{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "\n",
    "Let the parameter space $S=[0,1] \\times \\mathbb{R}^2 \\times \\mathbb{R}_{++}^2$ and $\\lambda = \\begin{pmatrix} p & \\mu_1 & \\mu_2 & \\sigma_1 & \\sigma_2 \\end{pmatrix}^T \\in S$. Use:\n",
    "\n",
    "- $\\lambda^0 = \\begin{pmatrix} 0.88 & 3.79 & 32.64 & 8.08 & 7.39 \\end{pmatrix}^T$\n",
    "- Fixed step-size $\\bar{t} = 0.002$\n",
    "- Stopping criterion $\\epsilon=10^{-5}$\n",
    "- Maximum number of iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part reads \"data.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"data.csv\", header=None)\n",
    "df = pd.DataFrame(data).values.tolist()\n",
    "X = [val for sublist in df for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function is defined to calculate the value of a normal pdf\n",
    "def normalpdf(x,mu,sigma):\n",
    "    return math.exp(-((x-mu)**2)/(2*sigma**2))/math.sqrt(2*math.pi*sigma**2)\n",
    "\n",
    "def f(x,p,mu1,mu2,sigma1,sigma2):\n",
    "    return p*normalpdf(x,mu1,sigma1)+(1-p)*normalpdf(x,mu2,sigma2)\n",
    "\n",
    "def loglikelihood_f(X,p,mu1,mu2,sigma1,sigma2):\n",
    "    sum = 0\n",
    "    for x in X:\n",
    "        sum += math.log(f(x,p,mu1,mu2,sigma1,sigma2))\n",
    "    return sum\n",
    "\n",
    "# Functions are defined for each component of the gradient\n",
    "def partialp(X,p,mu1,mu2,sigma1,sigma2):\n",
    "    s = 0\n",
    "    for x in X:\n",
    "        s +=  1/f(x,p,mu1,mu2,sigma1,sigma2)*(normalpdf(x,mu1,sigma1)-normalpdf(x,mu2,sigma2))        \n",
    "    return s    \n",
    "\n",
    "def partialmu1(X,p,mu1,mu2,sigma1,sigma2):\n",
    "    s = 0\n",
    "    for x in X:\n",
    "        s += 1/f(x,p,mu1,mu2,sigma1,sigma2)*p*normalpdf(x,mu1,sigma1)*(x-mu1)/sigma1**2\n",
    "    return s\n",
    "\n",
    "def partialmu2(X,p,mu1,mu2,sigma1,sigma2):\n",
    "    s = 0\n",
    "    for x in X:\n",
    "        s += 1/f(x,p,mu1,mu2,sigma1,sigma2)*(1-p)*normalpdf(x,mu2,sigma2)*(x-mu2)/sigma2**2\n",
    "    return s\n",
    "\n",
    "def partialsigma1(X,p,mu1,mu2,sigma1,sigma2):\n",
    "    s = 0\n",
    "    for x in X:\n",
    "        s += 1/f(x,p,mu1,mu2,sigma1,sigma2)*p*normalpdf(x,mu1,sigma1)*((x-mu1)**2-sigma1**2)/abs(sigma1)**3\n",
    "    return s\n",
    "\n",
    "def partialsigma2(X,p,mu1,mu2,sigma1,sigma2):\n",
    "    s = 0\n",
    "    for x in X:\n",
    "        s += 1/f(x,p,mu1,mu2,sigma1,sigma2)*(1-p)*normalpdf(x,mu2,sigma2)*((x-mu2)**2-sigma2**2)/abs(sigma2)**3\n",
    "    return s\n",
    "\n",
    "# A function is defined to calculate the gradient\n",
    "def grad(X,p,mu1,mu2,sigma1,sigma2):\n",
    "    return np.array([partialp(X,p,mu1,mu2,sigma1,sigma2),partialmu1(X,p,mu1,mu2,sigma1,sigma2),partialmu2(X,p,mu1,mu2,sigma1,sigma2),partialsigma1(X,p,mu1,mu2,sigma1,sigma2),partialsigma2(X,p,mu1,mu2,sigma1,sigma2)])\n",
    "\n",
    "# A function is defined to calculate the projection operator\n",
    "def proj(p,lb,ub):\n",
    "    if p > ub:\n",
    "        p = ub\n",
    "    elif p < lb:\n",
    "        p = lb\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  4 ; The maximum likelihood estimates of lambda = [ 0.69448322  3.79385472 32.64005472  8.09087593  7.39349867]\n"
     ]
    }
   ],
   "source": [
    "# a) Ordinary gradient descent method\n",
    "\n",
    "# Obtain the initial estimates\n",
    "p = 0.88\n",
    "mu1 = 3.79\n",
    "mu2 = 32.64\n",
    "sigma1 = 8.08\n",
    "sigma2 = 7.39\n",
    "\n",
    "# Set stepsize and stopping criteria as prescribed\n",
    "tolerance = 10**(-5)\n",
    "stepsize = 0.002\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while np.linalg.norm(grad(X,p,mu1,mu2,sigma1,sigma2)) > tolerance and iteration <= 10000:\n",
    "    \n",
    "    if iteration % 500 == 0 and iteration != 0:\n",
    "        print('Iteration ',iteration,'; lambda =', np.array([p,mu1,mu2,sigma1,sigma2]))\n",
    "        \n",
    "    ascent = np.array([stepsize*partialp(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialmu1(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialmu2(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialsigma1(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialsigma2(X,p,mu1,mu2,sigma1,sigma2)])\n",
    "    \n",
    "    if p + ascent[0] < 0 or p + ascent[0] > 1:\n",
    "        break\n",
    "    else:\n",
    "        p += ascent[0]\n",
    "        mu1 += ascent[1]\n",
    "        mu2 += ascent[2]\n",
    "        sigma1 += ascent[3]\n",
    "        sigma2 += ascent[4]\n",
    "        iteration += 1     \n",
    "\n",
    "if iteration <= 10000:\n",
    "    print('Iteration ',iteration,'; The maximum likelihood estimates of lambda =',np.array([p,mu1,mu2,sigma1,sigma2]))\n",
    "else:\n",
    "    print('The algorithm does not converge after 10000 iterations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p is defined to be in interval [0,1], the maximum likelihood estimates of $\\lambda$ using the ordinary gradient descent is achieved at the 5th iteration. An unconstrained maximization will cause p to exceed its interval. Therefore, gradient projection method is used below to pull estimate of p back in interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500 ; lambda = [ 0.86411864  4.77121603 27.14015431  9.52653418 16.72969304]\n",
      "Iteration  1000 ; lambda = [ 0.8364927   3.94865996 26.97772485  8.37889322 16.21649142]\n",
      "Iteration  1330 ; The maximum likelihood estimates of lambda = [ 0.83106263  3.70358377 26.8355153   8.21663163 15.84735075]\n"
     ]
    }
   ],
   "source": [
    "# b) Gradient projection method; interval [0,1]; stepsize 0.002\n",
    "\n",
    "# Obtain the initial estimates\n",
    "p = 0.88\n",
    "mu1 = 3.79\n",
    "mu2 = 32.64\n",
    "sigma1 = 8.08\n",
    "sigma2 = 7.39\n",
    "\n",
    "# Set stepsize and stopping criteria as prescribed\n",
    "tolerance = 10**(-5)\n",
    "stepsize = 0.002\n",
    "\n",
    "iteration = 0\n",
    "old_lamb = np.zeros(5)\n",
    "new_lamb = np.array([p,mu1,mu2,sigma1,sigma2])\n",
    "\n",
    "while np.linalg.norm(old_lamb[0] - new_lamb[0]) > tolerance and iteration <= 10000:\n",
    "    old_lamb = np.array([p,mu1,mu2,sigma1,sigma2])\n",
    "    \n",
    "    if iteration % 500 == 0 and iteration != 0:\n",
    "        print('Iteration ',iteration,'; lambda =', old_lamb)\n",
    "        \n",
    "    ascent = np.array([stepsize*partialp(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialmu1(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialmu2(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialsigma1(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialsigma2(X,p,mu1,mu2,sigma1,sigma2)])\n",
    "    \n",
    "    p = proj(p + ascent[0],0,1)\n",
    "    mu1 += ascent[1]\n",
    "    mu2 += ascent[2]\n",
    "    sigma1 += ascent[3]\n",
    "    sigma2 += ascent[4]\n",
    "    iteration += 1\n",
    "    \n",
    "    new_lamb = np.array([p,mu1,mu2,sigma1,sigma2])\n",
    "\n",
    "if iteration <= 10000:\n",
    "    print('Iteration ',iteration,'; The maximum likelihood estimates of lambda =',np.array([p,mu1,mu2,sigma1,sigma2]))\n",
    "else:\n",
    "    print('The algorithm does not converge after 10000 iterations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By pulling the estimate of p back in the interval of [0,1], the maximum likelihood estimates of $\\lambda$ converge at iteration 1330.With a better fit of the dataset, the estimates of p and $\\sigma_2$ are significantly larger than the unconstrained case, which indicates a larger volume of data fitting better on the left distribution of the normal-normal distribution and a greater spread of data on the right distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500 ; lambda = [ 0.84585491  3.93913143 32.56924741  8.32956049  7.77789103]\n",
      "Iteration  1000 ; lambda = [ 0.84231545  3.94020277 32.48367984  8.32496794  8.00703849]\n",
      "Iteration  1500 ; lambda = [ 0.83865007  3.92233621 32.37898026  8.31254069  8.16944951]\n",
      "Iteration  2000 ; lambda = [ 0.83523705  3.90375572 32.26491699  8.30030707  8.29592936]\n",
      "Iteration  2500 ; lambda = [ 0.83203342  3.88616521 32.14720539  8.28876115  8.401302  ]\n",
      "Iteration  3000 ; lambda = [ 0.828986    3.86948153 32.02881492  8.27785009  8.49366268]\n"
     ]
    }
   ],
   "source": [
    "# c) Gradient projection method; interval [0.1,0.9]; stepsize 0.002\n",
    "\n",
    "# Obtain the initial estimates\n",
    "p = 0.88\n",
    "mu1 = 3.79\n",
    "mu2 = 32.64\n",
    "sigma1 = 8.08\n",
    "sigma2 = 7.39\n",
    "\n",
    "# Set stepsize and stopping criteria as prescribed\n",
    "tolerance = 10**(-5)\n",
    "stepsize = 0.002\n",
    "\n",
    "iteration = 0\n",
    "old_lamb = np.zeros(5)\n",
    "new_lamb = np.array([p,mu1,mu2,sigma1,sigma2])\n",
    "\n",
    "while np.linalg.norm(old_lamb[0] - new_lamb[0]) > tolerance and iteration <= 10000:\n",
    "    old_lamb = np.array([p,mu1,mu2,sigma1,sigma2])\n",
    "    \n",
    "    if iteration % 500 == 0 and iteration != 0:\n",
    "        print('Iteration ',iteration,'; lambda =', old_lamb)\n",
    "        \n",
    "    ascent = np.array([stepsize*partialp(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialmu1(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialmu2(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialsigma1(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialsigma2(X,p,mu1,mu2,sigma1,sigma2)])\n",
    "    \n",
    "    p = proj(p + ascent[0],0.1,0.9)\n",
    "    mu1 += ascent[1]\n",
    "    mu2 += ascent[2]\n",
    "    sigma1 += ascent[3]\n",
    "    sigma2 += ascent[4]\n",
    "    iteration += 1\n",
    "    \n",
    "    new_lamb = np.array([p,mu1,mu2,sigma1,sigma2])\n",
    "\n",
    "if iteration <= 10000:\n",
    "    print('Iteration ',iteration,'; The maximum likelihood estimates of lambda =',np.array([p,mu1,mu2,sigma1,sigma2]))\n",
    "else:\n",
    "    print('The algorithm does not converge after 10000 iterations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By constraining the interval of the estimate of p to [0.1,0.9], the maximum likelihood estimates of $\\lambda$ do not converge. This is because a stepsize of 0.002 is larger than a stepsize of $\\frac{2}{L}$ where L is the Lipschitz constant of $\\bigtriangledown f$, causing an oscillating behaviour within the algorithm due to the excessive increase of the constant stepsize gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Gradient projection method; interval [0.1,0.9]; stepsize 0.001\n",
    "\n",
    "# Obtain the initial estimates\n",
    "p = 0.88\n",
    "mu1 = 3.79\n",
    "mu2 = 32.64\n",
    "sigma1 = 8.08\n",
    "sigma2 = 7.39\n",
    "\n",
    "# Set stepsize and stopping criteria as prescribed\n",
    "tolerance = 10**(-5)\n",
    "stepsize = 0.001\n",
    "\n",
    "iteration = 0\n",
    "old_lamb = np.zeros(5)\n",
    "new_lamb = np.array([p,mu1,mu2,sigma1,sigma2])\n",
    "\n",
    "while np.linalg.norm(old_lamb[0] - new_lamb[0]) > tolerance and iteration <= 10000:\n",
    "    old_lamb = np.array([p,mu1,mu2,sigma1,sigma2])\n",
    "    \n",
    "    if iteration % 500 == 0 and iteration != 0:\n",
    "        print('Iteration ',iteration,'; lambda =', old_lamb)\n",
    "        \n",
    "    ascent = np.array([stepsize*partialp(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialmu1(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialmu2(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialsigma1(X,p,mu1,mu2,sigma1,sigma2),\n",
    "                       stepsize*partialsigma2(X,p,mu1,mu2,sigma1,sigma2)])\n",
    "    \n",
    "    p = proj(p + ascent[0],0,1)\n",
    "    mu1 += ascent[1]\n",
    "    mu2 += ascent[2]\n",
    "    sigma1 += ascent[3]\n",
    "    sigma2 += ascent[4]\n",
    "    iteration += 1\n",
    "    \n",
    "    new_lamb = np.array([p,mu1,mu2,sigma1,sigma2])\n",
    "\n",
    "if iteration <= 10000:\n",
    "    print('Iteration ',iteration,'; The maximum likelihood estimates of lambda =',new_lamb)\n",
    "else:\n",
    "    print('The algorithm does not converge after 10000 iterations.')\n",
    "    \n",
    "print('The log likelihood is',loglikelihood_f(X,p,mu1,mu2,sigma1,sigma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reducing the stepsize to 0.001, the maximum likelihood estimates of $\\lambda$  at p $\\in$ [0.1, 0.9] converge at iteration 18. Hence, this stepsize contributes to the sufficient decrease of the gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "\n",
    "a), b) and c) are available in pdf (\"20311298_Assignment 2\")\n",
    "\n",
    "d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_affine(x,A,b):\n",
    "    return x - np.transpose(A)@np.linalg.inv(A@np.transpose(A))@(A@x-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient projection method\n",
    "\n",
    "# Obtain the initial estimates\n",
    "lamb = np.array([[5],[5],[5],[5],[5]])\n",
    "stepsize = 0.005\n",
    "grad = np.array([[-120],[-12],[800],[2],[2]])\n",
    "iteration = 0\n",
    "\n",
    "A = np.array([[6.5,1,-45,-1,0],[7.8,1,-60,0,-1],[9.2,1,-75,0,0]])\n",
    "b = np.array([[150],[175],[218]])\n",
    "\n",
    "while iteration <= 400000:\n",
    "\n",
    "    if iteration % 20000 == 0 and iteration != 0:\n",
    "        print('Iteration ',iteration,'; lambda =', lamb)\n",
    "    \n",
    "    lamb = proj_affine(lamb + stepsize*grad,A,b)\n",
    "    for i in range(len(lamb)):\n",
    "        lamb[i] = max(0,lamb[i])\n",
    "\n",
    "    iteration += 1\n",
    "    \n",
    "profit = (-120*lamb[0] - 12*lamb[1] + 800*lamb[2] + 2*lamb[3] + 2*lamb[4])*1000\n",
    "print('The dual solution is',profit)\n",
    "print('The difference between primal and dual is', -2394000 - profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ from Primal:\n",
    "\n",
    "$\\lambda = \\begin{bmatrix}\n",
    "0 & 218 & 0 & 68 & 43\n",
    "\\end{bmatrix}^T$\n",
    "\n",
    "\n",
    "$\\lambda$ from Dual at stepsize = 0.005:\n",
    "\n",
    "$\\lambda = \\begin{bmatrix}\n",
    "0 & 215.0688 & 0 & 66.7403 & 42.3746\n",
    "\\end{bmatrix}^T$\n",
    "\n",
    "$\\lambda$ from Dual at stepsize = 0.002:\n",
    "\n",
    "$\\lambda = \\begin{bmatrix}\n",
    "0 & 216.82752 & 0 & 67.49612 & 42.74984\n",
    "\\end{bmatrix}^T$\n",
    "\n",
    "Similar answer is obtained compared to b), we can improve the precision by applying backtracking instead of constant stepsize. Backtracking ensures iteration of stepsize to obtain suboptimal stepsize until achieving the Sufficient Decrease Property. This guarantees the improvement in the maximization in each iteration with a decreasing stepsize to achieve faster and more accurate convergence. We can also observe that stepsize = 0.002 produces an answer closer to $\\lambda$ from Primal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
